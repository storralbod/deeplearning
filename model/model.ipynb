{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('processed_marginal_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('processed_marginal_data_with_year_scaled.csv')\n",
    "\n",
    "# Experiment with a longer sequence length (e.g., 336 hours for 2 weeks)\n",
    "sequence_length = 336  # 2 weeks\n",
    "\n",
    "# Prepare the input features (include new parameters) and the target price\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(data) - sequence_length):\n",
    "    features = data.iloc[i:i+sequence_length][[\n",
    "        'Price1', 'Day_Sin', 'Day_Cos', 'Month_Sin', 'Month_Cos', 'Year_Scaled'\n",
    "    ]].values.flatten()  # Flatten to make a single sequence of features\n",
    "    X.append(features)\n",
    "    y.append(data.iloc[i + sequence_length]['Price1'])  # Target is the price after the sequence\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, sequence_length, 6)  # Reshape input for LSTM (batch_size, sequence_length, feature_dim)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Only take the output of the last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize both models\n",
    "input_size = X_train.shape[1]  # Flattened input size for MLP\n",
    "mlp_model = MLP(input_size)\n",
    "lstm_model = LSTM(input_size=6, hidden_size=4, num_layers=2)  # LSTM input size is feature_dim (3)\n",
    "\n",
    "# Define loss and optimizer for both models\n",
    "criterion = nn.MSELoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, optimizer, epochs=100):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val)\n",
    "            val_loss = criterion(val_output, y_val)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP model...\n",
      "Epoch [10/100], Loss: 8833.6016, Val Loss: 8632.0293\n",
      "Epoch [20/100], Loss: 4835.5513, Val Loss: 4739.0796\n",
      "Epoch [30/100], Loss: 4947.0073, Val Loss: 4942.9165\n",
      "Epoch [40/100], Loss: 4443.1567, Val Loss: 4560.8403\n",
      "Epoch [50/100], Loss: 4461.2026, Val Loss: 4544.2295\n",
      "Epoch [60/100], Loss: 4409.4160, Val Loss: 4491.8379\n",
      "Epoch [70/100], Loss: 4376.0215, Val Loss: 4463.5293\n",
      "Epoch [80/100], Loss: 4359.3477, Val Loss: 4447.1230\n",
      "Epoch [90/100], Loss: 4337.6328, Val Loss: 4421.7051\n",
      "Epoch [100/100], Loss: 4315.2710, Val Loss: 4401.1050\n"
     ]
    }
   ],
   "source": [
    "print(\"Training MLP model...\")\n",
    "trained_mlp = train_model(mlp_model, mlp_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch [10/100], Loss: 11331.9121, Val Loss: 11569.7314\n",
      "Epoch [20/100], Loss: 11317.9121, Val Loss: 11555.6260\n",
      "Epoch [30/100], Loss: 11304.5000, Val Loss: 11542.1279\n",
      "Epoch [40/100], Loss: 11291.3389, Val Loss: 11528.8096\n",
      "Epoch [50/100], Loss: 11277.4023, Val Loss: 11514.5869\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LSTM model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trained_lstm \u001b[38;5;241m=\u001b[39m train_model(lstm_model, lstm_optimizer)\n",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y_train)\n\u001b[0;32m----> 9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training LSTM model...\")\n",
    "trained_lstm = train_model(lstm_model, lstm_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, X, y, model_name=\"\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).numpy()\n",
    "        true_values = y.numpy()\n",
    "        test_loss = criterion(torch.tensor(predictions), torch.tensor(true_values))\n",
    "        print(f\"{model_name} Test Loss: {test_loss.item():.4f}\")\n",
    "        return predictions, true_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Test Loss: 1513.9897\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trained_lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate both models on the test set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mlp_predictions, mlp_true_values \u001b[38;5;241m=\u001b[39m evaluate_model(trained_mlp, X_test, y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m lstm_predictions, lstm_true_values \u001b[38;5;241m=\u001b[39m evaluate_model(trained_lstm, X_test, y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot predictions vs. true values for both models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_lstm' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate both models on the test set\n",
    "mlp_predictions, mlp_true_values = evaluate_model(trained_mlp, X_test, y_test, \"MLP\")\n",
    "lstm_predictions, lstm_true_values = evaluate_model(trained_lstm, X_test, y_test, \"LSTM\")\n",
    "\n",
    "# Plot predictions vs. true values for both models\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp_true_values, label=\"True Values\", color='blue')\n",
    "plt.plot(mlp_predictions, label=\"MLP Predictions\", color='red')\n",
    "plt.legend()\n",
    "plt.title(\"MLP Predictions vs. True Values\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Price\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lstm_true_values, label=\"True Values\", color='blue')\n",
    "plt.plot(lstm_predictions, label=\"LSTM Predictions\", color='red')\n",
    "plt.legend()\n",
    "plt.title(\"LSTM Predictions vs. True Values\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Price\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2024-11-11 21:26:19.509313: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18966528 exceeds 10% of free system memory.\n",
      "2024-11-11 21:26:19.529963: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18966528 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1731356782.496630 1396632 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-11-11 21:26:22.499239: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at cudnn_rnn_ops.cc:1769 : INVALID_ARGUMENT: Dnn is not supported\n",
      "2024-11-11 21:26:22.499337: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Dnn is not supported\n",
      "\t [[{{function_node __inference_one_step_on_data_5783}}{{node sequential_3_1/lstm_3_1/CudnnRNNV3}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_3_1/lstm_3_1/CudnnRNNV3 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_590439/2322252996.py\", line 51, in <module>\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 51, in train_step\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/functional.py\", line 584, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py\", line 570, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py\", line 537, in inner_loop\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/rnn.py\", line 933, in _cudnn_lstm\n\nDnn is not supported\n\t [[{{node sequential_3_1/lstm_3_1/CudnnRNNV3}}]] [Op:__inference_one_step_on_iterator_5824]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Adjust based on your needs\u001b[39;00m\n\u001b[1;32m     49\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 51\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, y_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearn/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_3_1/lstm_3_1/CudnnRNNV3 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_590439/2322252996.py\", line 51, in <module>\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 51, in train_step\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 213, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/models/functional.py\", line 584, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py\", line 570, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/layers/rnn/lstm.py\", line 537, in inner_loop\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/rnn.py\", line 841, in lstm\n\n  File \"/home/teitur/miniconda3/envs/deeplearn/lib/python3.12/site-packages/keras/src/backend/tensorflow/rnn.py\", line 933, in _cudnn_lstm\n\nDnn is not supported\n\t [[{{node sequential_3_1/lstm_3_1/CudnnRNNV3}}]] [Op:__inference_one_step_on_iterator_5824]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('/home/teitur/DTU/electricproject/deeplearning/model/merged_marginal_data.csv')\n",
    "\n",
    "# Define input and output sequence lengths\n",
    "sequence_length = 336  # Use 2 weeks of history for predictions\n",
    "output_sequence_length = 24  # Predict the next 24 hours\n",
    "\n",
    "# Prepare the input features and target prices\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(data) - sequence_length - output_sequence_length):\n",
    "    # Use 'Price1' and any other relevant features you have added\n",
    "    features = data[['Price1', 'Hour_Sin', 'Hour_Cos']].values\n",
    "    X.append(features[i:i + sequence_length])  # input sequence\n",
    "    y.append(features[i + sequence_length:i + sequence_length + output_sequence_length, 0])  # output sequence of prices\n",
    "\n",
    "X = np.array(X)  # Shape: (samples, sequence_length, features)\n",
    "y = np.array(y)  # Shape: (samples, output_sequence_length)\n",
    "\n",
    "# Normalize data\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_shape = X.shape\n",
    "X = scaler_X.fit_transform(X.reshape(-1, X.shape[2])).reshape(X_shape)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(units=output_sequence_length)  # Output layer for sequence prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "epochs = 50  # Adjust based on your needs\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Train Loss: {train_loss:.6f}')\n",
    "print(f'Test Loss: {test_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the most recent data for prediction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m recent_data \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, sequence_length, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# Shape it correctly for LSTM\u001b[39;00m\n\u001b[1;32m      3\u001b[0m predicted_sequence \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(recent_data)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Inverse transform the predictions to get actual prices\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Use the most recent data for prediction\n",
    "recent_data = X[-1].reshape(1, sequence_length, X.shape[2])  # Shape it correctly for LSTM\n",
    "predicted_sequence = model.predict(recent_data)\n",
    "\n",
    "# Inverse transform the predictions to get actual prices\n",
    "predicted_prices = scaler_y.inverse_transform(predicted_sequence).flatten()\n",
    "\n",
    "# Print predicted prices for the next 24 hours\n",
    "print(\"Predicted Prices for the Next 24 Hours:\", predicted_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      5\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_test_pred)\n\u001b[1;32m      6\u001b[0m y_test_actual \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred = scaler_y.inverse_transform(y_test_pred)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Plot predictions vs. actual values for one test example\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_actual[0], label=\"Actual Prices\", color=\"blue\")\n",
    "plt.plot(y_test_pred[0], label=\"Predicted Prices\", color=\"red\")\n",
    "plt.title(\"Predicted vs. Actual Prices for Next 24 Hours\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Available GPU(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Num GPUs Available:\", len(gpus))\n",
    "    print(\"Available GPU(s):\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

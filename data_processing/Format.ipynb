{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teitur/DTU/electricproject/deeplearning/data_processing\n",
      "Found 2 marginalpdbc files\n",
      "Found 2 precious files\n",
      "Found 6 gas price files\n",
      "Found 6 gas price files\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2023.csv\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2021.csv\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2018.csv\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2020.csv\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2019.csv\n",
      "Processing file: ../raw_data/data_gas/MIBGAS_Data_2022.csv\n",
      "Combined gas data rows after filling: 2089\n",
      "      Year  Month  Day  Gas_Price\n",
      "1421  2018      1    1      24.50\n",
      "1420  2018      1    2      23.95\n",
      "1419  2018      1    3      21.90\n",
      "1418  2018      1    4      19.63\n",
      "1417  2018      1    5      19.85\n",
      "Processing file: ../raw_data/data_electric/data_da_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_779085/2245254277.py:57: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  gas_data['Gas_Price'] = gas_data['Gas_Price'].fillna(method='ffill')\n",
      "/tmp/ipykernel_779085/2245254277.py:58: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  gas_data['Gas_Price'] = gas_data['Gas_Price'].fillna(method='bfill')  # Backfill if forward-fill doesn't work\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: [1, 2, 3, 4, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m marginalpdbc_files:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     73\u001b[0m     data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     74\u001b[0m     data \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice1\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:155\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(\n\u001b[1;32m    156\u001b[0m             usecols,\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_parse_dates_presence(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:979\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    977\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 979\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    982\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: [1, 2, 3, 4, 5]"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#print working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "data_dir = '../raw_data/'\n",
    "output_dir = '../formatted_data/'\n",
    "\n",
    "# Lets print how many files are in the raw_data directory\n",
    "\n",
    "marginalpdbc_files = glob.glob(os.path.join(data_dir, 'data_electric/data_da*.csv'))\n",
    "precious_files = glob.glob(os.path.join(data_dir, 'data_electric/data_id*.csv'))\n",
    "gas_files = glob.glob(os.path.join(data_dir, 'data_gas/MIBGAS_Data_*.csv'))\n",
    "\n",
    "print(f\"Found {len(marginalpdbc_files)} marginalpdbc files\")\n",
    "print(f\"Found {len(precious_files)} precious files\")\n",
    "print(f\"Found {len(gas_files)} gas price files\")      \n",
    "\n",
    "# Function to encode time (day, month, hour) as sine and cosine\n",
    "def encode_time(value, max_value):\n",
    "    value_sin = np.sin(2 * np.pi * value / max_value)\n",
    "    value_cos = np.cos(2 * np.pi * value / max_value)\n",
    "    return value_sin, value_cos\n",
    "\n",
    "\n",
    "# Load and preprocess all gas price data\n",
    "all_gas_data = []\n",
    "print(f\"Found {len(gas_files)} gas price files\")\n",
    "\n",
    "for gas_file in gas_files:\n",
    "    print(f\"Processing file: {gas_file}\")\n",
    "    try:\n",
    "        gas_data = pd.read_csv(gas_file)\n",
    "        gas_data.columns = gas_data.columns.str.strip().str.replace('\\n', ' ', regex=False)  # Normalize column names\n",
    "        gas_data = gas_data[['Trading Day', 'Price [EUR/MWh]']].rename(columns={\n",
    "            'Trading Day': 'Trading_Day',\n",
    "            'Price [EUR/MWh]': 'Gas_Price'\n",
    "        })\n",
    "        gas_data['Trading_Day'] = pd.to_datetime(gas_data['Trading_Day'])\n",
    "        gas_data['Year'] = gas_data['Trading_Day'].dt.year\n",
    "        gas_data['Month'] = gas_data['Trading_Day'].dt.month\n",
    "        gas_data['Day'] = gas_data['Trading_Day'].dt.day\n",
    "        all_gas_data.append(gas_data[['Year', 'Month', 'Day', 'Gas_Price']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {gas_file}: {e}\")\n",
    "\n",
    "# Combine all gas price data\n",
    "gas_data = pd.concat(all_gas_data, ignore_index=True)\n",
    "gas_data = gas_data.drop_duplicates(subset=['Year', 'Month', 'Day'])\n",
    "\n",
    "\n",
    "# Forward-fill missing Gas_Price values\n",
    "gas_data = gas_data.sort_values(by=['Year', 'Month', 'Day'])\n",
    "gas_data['Gas_Price'] = gas_data['Gas_Price'].fillna(method='ffill')\n",
    "gas_data['Gas_Price'] = gas_data['Gas_Price'].fillna(method='bfill')  # Backfill if forward-fill doesn't work\n",
    "\n",
    "print(f\"Combined gas data rows after filling: {len(gas_data)}\")\n",
    "print(gas_data.head())\n",
    "\n",
    "# Process marginalpdbc files and merge all data\n",
    "marginal_data = []\n",
    "\n",
    "# Example of marginalpdbc data\n",
    "# Year,Month,Day,Hour,DA ES\n",
    "# 2023.0,1.0,1.0,1.0,0.0\n",
    "\n",
    "for file in marginalpdbc_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    data = pd.read_csv(file, delimiter=';', header=None, skiprows=1, usecols=range(6), encoding='latin1').iloc[:-1, :]\n",
    "    data.columns = ['Year', 'Month', 'Day', 'Hour', 'Price1', 'Price2']\n",
    "    data = data[['Year', 'Month', 'Day', 'Hour', 'Price1']].dropna()\n",
    "    data['Hour'] = pd.to_numeric(data['Hour'], errors='coerce')\n",
    "    data['Month'] = pd.to_numeric(data['Month'], errors='coerce')\n",
    "    data['Day'] = pd.to_numeric(data['Day'], errors='coerce')\n",
    "    data = data.dropna(subset=['Hour', 'Month', 'Day'])\n",
    "    \n",
    "    # Encode time features\n",
    "    data['Hour_Sin'], data['Hour_Cos'] = zip(*data['Hour'].apply(lambda x: encode_time(x, 24)))\n",
    "    data['Day_Sin'], data['Day_Cos'] = zip(*data['Day'].apply(lambda x: encode_time(x, 31)))\n",
    "    data['Month_Sin'], data['Month_Cos'] = zip(*data['Month'].apply(lambda x: encode_time(x, 12)))\n",
    "\n",
    "    # Ensure Year is numeric and scaled\n",
    "    data['Year'] = data['Year'].astype(int)\n",
    "    data['Year_Scaled'] = (data['Year'] - 2018) * 0.1 + 0.1\n",
    "\n",
    "    # Merge with gas price data\n",
    "    merged_data = pd.merge(data, gas_data, on=['Year', 'Month', 'Day'], how='left')\n",
    "\n",
    "    marginal_data.append(merged_data)\n",
    "\n",
    "# Combine all marginal data\n",
    "marginal_data = pd.concat(marginal_data, ignore_index=True)\n",
    "marginal_data = marginal_data.drop_duplicates(subset=['Year', 'Month', 'Day', 'Hour'])\n",
    "\n",
    "# Process precious files and merge with marginal data\n",
    "precious_data = []\n",
    "\n",
    "# Example of precious data\n",
    "# Year,Month,Day,Hour,MaxES,MinES,AvgES\n",
    "# 2023.0,1.0,1.0,1.0,0.14,-4.0,-0.72\n",
    "\n",
    "for file in precious_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    try:\n",
    "        data = pd.read_csv(file, delimiter=';', header=None, skiprows=1, usecols=range(6), encoding='latin1').iloc[:-1, :]\n",
    "        data.columns = ['Year', 'Month', 'Day', 'Hour', 'Price_Precious', 'Other_Column']\n",
    "        data = data[['Year', 'Month', 'Day', 'Hour', 'Price_Precious']].dropna()\n",
    "        data['Hour'] = pd.to_numeric(data['Hour'], errors='coerce')\n",
    "        data['Month'] = pd.to_numeric(data['Month'], errors='coerce')\n",
    "        data['Day'] = pd.to_numeric(data['Day'], errors='coerce')\n",
    "        # Replace commas with dots and convert to numeric\n",
    "        data['Price_Precious'] = pd.to_numeric(data['Price_Precious'].str.replace(',', '.'), errors='coerce')\n",
    "        data = data.dropna(subset=['Hour', 'Month', 'Day'])\n",
    "\n",
    "        # Encode time features\n",
    "        data['Hour_Sin'], data['Hour_Cos'] = zip(*data['Hour'].apply(lambda x: encode_time(x, 24)))\n",
    "        data['Day_Sin'], data['Day_Cos'] = zip(*data['Day'].apply(lambda x: encode_time(x, 31)))\n",
    "        data['Month_Sin'], data['Month_Cos'] = zip(*data['Month'].apply(lambda x: encode_time(x, 12)))\n",
    "\n",
    "        # Ensure Year is numeric and scaled\n",
    "        data['Year'] = data['Year'].astype(int)\n",
    "        data['Year_Scaled'] = (data['Year'] - 2018) * 0.1 + 0.1\n",
    "\n",
    "        # Merge with gas price data\n",
    "        merged_data = pd.merge(data, gas_data, on=['Year', 'Month', 'Day'], how='left')\n",
    "\n",
    "        precious_data.append(merged_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Combine all precious data\n",
    "precious_data = pd.concat(precious_data, ignore_index=True)\n",
    "precious_data = precious_data.drop_duplicates(subset=['Year', 'Month', 'Day', 'Hour'])\n",
    "\n",
    "# Merge precious data with marginal data using inner join to ensure consistency\n",
    "final_data = pd.merge(\n",
    "    marginal_data,\n",
    "    precious_data,\n",
    "    on=[\n",
    "        'Year', 'Month', 'Day', 'Hour', 'Hour_Sin', 'Hour_Cos',\n",
    "        'Day_Sin', 'Day_Cos', 'Month_Sin', 'Month_Cos', 'Year_Scaled',\n",
    "        'Gas_Price'\n",
    "    ],\n",
    "    how='inner'  # Use 'inner' join to keep only consistent timestamps\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "final_data = final_data.rename(columns={\n",
    "    'Price1': 'DA',  # Day Ahead prices\n",
    "    'Price_Precious': 'ID'  # Intra Day prices\n",
    "})\n",
    "\n",
    "# Add a new column for the price difference\n",
    "final_data['Diff'] = final_data['DA'] - final_data['ID']\n",
    "\n",
    "# Save two versions of the data\n",
    "\n",
    "# 1. Version with original time columns\n",
    "columns_with_time = [\n",
    "    'Year', 'Month', 'Day', 'Hour', 'DA', 'ID', 'Diff',\n",
    "    'Hour_Sin', 'Hour_Cos', 'Day_Sin', 'Day_Cos', 'Month_Sin', \n",
    "    'Month_Cos', 'Year_Scaled', 'Gas_Price'\n",
    "]\n",
    "data_with_time = final_data[columns_with_time]\n",
    "data_with_time = data_with_time.sort_values(by=['Year', 'Month', 'Day', 'Hour'])\n",
    "# We use the output directory we defined earlier\n",
    "output_file_with_time = output_dir + 'final_data_with_time.csv'\n",
    "data_with_time.to_csv(output_file_with_time, index=False)\n",
    "print(f\"Data with time columns has been processed and saved to '{output_file_with_time}'\")\n",
    "\n",
    "# 2. Version with only encoded values\n",
    "columns_encoded_only = [\n",
    "    'DA', 'ID', 'Diff', 'Hour_Sin', 'Hour_Cos', \n",
    "    'Day_Sin', 'Day_Cos', 'Month_Sin', 'Month_Cos', \n",
    "    'Year_Scaled', 'Gas_Price'\n",
    "]\n",
    "data_encoded_only = final_data[columns_encoded_only]\n",
    "output_file_encoded_only = output_dir + 'final_data_encoded_only.csv'\n",
    "data_encoded_only.to_csv(output_file_encoded_only, index=False)\n",
    "print(f\"Encoded data only has been processed and saved to '{output_file_encoded_only}'\")\n",
    "\n",
    "# Print sample outputs for verification\n",
    "print(\"Sample data with time columns:\")\n",
    "print(data_with_time.head())\n",
    "print(\"Sample encoded-only data:\")\n",
    "print(data_encoded_only.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 marginal files\n",
      "Found 2 intra-day files\n",
      "Found 6 gas price files\n",
      "Processing marginal file: ../raw_data/data_electric/data_da_23.csv\n",
      "Processing marginal file: ../raw_data/data_electric/data_da_24.csv\n",
      "Processing intra-day file: ../raw_data/data_electric/data_id_24.csv\n",
      "Processing intra-day file: ../raw_data/data_electric/data_id_23.csv\n",
      "Processing gas price files...\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Columns in gas data: Index(['Trading Day', 'Product', 'Delivery Zone', 'Gas type',\n",
      "       'Price [EUR/MWh]', 'Volume  [MWh]'],\n",
      "      dtype='object')\n",
      "Gas data processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_779085/217039046.py:97: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_gas_data['Price_EUR_MWh'] = combined_gas_data['Price_EUR_MWh'].fillna(method='ffill').fillna(method='bfill')\n",
      "/tmp/ipykernel_779085/217039046.py:98: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_gas_data['Volume_MWh'] = combined_gas_data['Volume_MWh'].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ../formatted_data/formatted_data.csv\n",
      "Missing values:\n",
      "Year                 0\n",
      "Month                0\n",
      "Day                  0\n",
      "Hour                 0\n",
      "DA                   0\n",
      "ID                   0\n",
      "Price_EUR_MWh    10415\n",
      "Volume_MWh       10415\n",
      "Hour_Sin             0\n",
      "Hour_Cos             0\n",
      "Day_Sin              0\n",
      "Day_Cos              0\n",
      "Month_Sin            0\n",
      "Month_Cos            0\n",
      "Year_Scaled          0\n",
      "Diff                 0\n",
      "dtype: int64\n",
      "Number of duplicate dates: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define directories\n",
    "data_dir = '../raw_data/'\n",
    "output_dir = '../formatted_data/'\n",
    "\n",
    "# Discover files\n",
    "marginal_files = glob.glob(os.path.join(data_dir, 'data_electric/data_da*.csv'))\n",
    "intra_day_files = glob.glob(os.path.join(data_dir, 'data_electric/data_id*.csv'))\n",
    "gas_files = glob.glob(os.path.join(data_dir, 'data_gas/MIBGAS_Data_*.csv'))\n",
    "\n",
    "print(f\"Found {len(marginal_files)} marginal files\")\n",
    "print(f\"Found {len(intra_day_files)} intra-day files\")\n",
    "print(f\"Found {len(gas_files)} gas price files\")\n",
    "\n",
    "# Time encoding function\n",
    "def encode_time(value, max_value):\n",
    "    value_sin = np.sin(2 * np.pi * value / max_value)\n",
    "    value_cos = np.cos(2 * np.pi * value / max_value)\n",
    "    return value_sin, value_cos\n",
    "\n",
    "# Function to process marginal data\n",
    "def process_marginal(file_path):\n",
    "    print(f\"Processing marginal file: {file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.rename(columns={'DA ES': 'DA'})\n",
    "        data = data[['Year', 'Month', 'Day', 'Hour', 'DA']].dropna()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing marginal file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process intra-day data\n",
    "def process_intra_day(file_path):\n",
    "    print(f\"Processing intra-day file: {file_path}\")\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.rename(columns={'AvgES': 'ID'})\n",
    "        data = data[['Year', 'Month', 'Day', 'Hour', 'ID']].dropna()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing intra-day file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process gas price data\n",
    "def process_gas(file_paths):\n",
    "    print(\"Processing gas price files...\")\n",
    "    all_gas_data = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            gas_data = pd.read_csv(file_path, delimiter=',')\n",
    "            \n",
    "            # Normalize column names by stripping spaces and removing newlines\n",
    "            gas_data.columns = gas_data.columns.str.strip().str.replace('\\n', ' ', regex=False)\n",
    "            \n",
    "            # Debugging: Print column names\n",
    "            print(\"Columns in gas data:\", gas_data.columns)\n",
    "            \n",
    "            # Rename columns based on expected order\n",
    "            gas_data = gas_data.rename(columns={\n",
    "                gas_data.columns[0]: 'Trading_Day',\n",
    "                gas_data.columns[4]: 'Price_EUR_MWh',\n",
    "                gas_data.columns[5]: 'Volume_MWh'\n",
    "            })\n",
    "            \n",
    "            # Convert 'Trading_Day' to datetime\n",
    "            gas_data['Trading_Day'] = pd.to_datetime(gas_data['Trading_Day'], errors='coerce')\n",
    "            \n",
    "            # Drop rows with invalid dates\n",
    "            gas_data = gas_data.dropna(subset=['Trading_Day'])\n",
    "            \n",
    "            # Extract Year, Month, and Day for merging\n",
    "            gas_data['Year'] = gas_data['Trading_Day'].dt.year\n",
    "            gas_data['Month'] = gas_data['Trading_Day'].dt.month\n",
    "            gas_data['Day'] = gas_data['Trading_Day'].dt.day\n",
    "            \n",
    "            # Keep only relevant columns\n",
    "            gas_data = gas_data[['Year', 'Month', 'Day', 'Price_EUR_MWh', 'Volume_MWh']]\n",
    "            \n",
    "            # Append to the list\n",
    "            all_gas_data.append(gas_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all gas data\n",
    "    combined_gas_data = pd.concat(all_gas_data, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    combined_gas_data = combined_gas_data.drop_duplicates(subset=['Year', 'Month', 'Day'])\n",
    "    \n",
    "    # Handle missing values\n",
    "    combined_gas_data['Price_EUR_MWh'] = combined_gas_data['Price_EUR_MWh'].fillna(method='ffill').fillna(method='bfill')\n",
    "    combined_gas_data['Volume_MWh'] = combined_gas_data['Volume_MWh'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    print(\"Gas data processed successfully.\")\n",
    "    return combined_gas_data\n",
    "\n",
    "# Process files\n",
    "marginal_data = pd.concat(\n",
    "    [df for file in marginal_files if (df := process_marginal(file)) is not None and not df.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "intra_day_data = pd.concat(\n",
    "    [df for file in intra_day_files if (df := process_intra_day(file)) is not None and not df.empty],\n",
    "    ignore_index=True\n",
    ")\n",
    "gas_data = process_gas(gas_files)\n",
    "\n",
    "# Merge marginal and intra-day data\n",
    "final_data = pd.merge(marginal_data, intra_day_data, on=['Year', 'Month', 'Day', 'Hour'], how='inner')\n",
    "\n",
    "# Optionally merge gas price data\n",
    "final_data = pd.merge(final_data, gas_data, on=['Year', 'Month', 'Day'], how='left')\n",
    "\n",
    "# Add encoded time columns\n",
    "final_data['Hour_Sin'], final_data['Hour_Cos'] = zip(*final_data['Hour'].apply(lambda x: encode_time(x, 24)))\n",
    "final_data['Day_Sin'], final_data['Day_Cos'] = zip(*final_data['Day'].apply(lambda x: encode_time(x, 31)))\n",
    "final_data['Month_Sin'], final_data['Month_Cos'] = zip(*final_data['Month'].apply(lambda x: encode_time(x, 12)))\n",
    "\n",
    "# Add scaled Year and price difference\n",
    "final_data['Year_Scaled'] = (final_data['Year'] - 2018) * 0.1 + 0.1\n",
    "final_data['Diff'] = final_data['DA'] - final_data['ID']\n",
    "\n",
    "# Save the processed data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, 'formatted_data.csv')\n",
    "final_data.to_csv(output_file, index=False)\n",
    "print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "# Print sample for verification\n",
    "# print(\"Sample of final data:\")\n",
    "# print(final_data.head())\n",
    "\n",
    "\n",
    "# Lets now go through the formatted_data.csv file and check if the data is correct\n",
    "\n",
    "final_data = pd.read_csv(output_file)\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = final_data.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Check for duplicate dates\n",
    "duplicate_dates = final_data.duplicated(subset=['Year', 'Month', 'Day', 'Hour'])\n",
    "\n",
    "# Print how many duplicates we have\n",
    "print(\"Number of duplicate dates:\", duplicate_dates.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

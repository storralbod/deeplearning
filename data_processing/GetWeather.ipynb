{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This one works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:15:23,286 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-12-09 16:15:23,287 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-12-09 16:15:23,288 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-12-09 16:15:23,289 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing month: 01...\n",
      "File temp_weather_data/2023/01/Madrid_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Barcelona_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Seville_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Valencia_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Castile-La Mancha (Wind)_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Aragon (Wind)_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Andalusia (Wind)_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Escatrón-Chiprana-Samper (Solar)_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Talasol Solar (Solar)_2023-01.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/01/Talayuela Solar (Solar)_2023-01.grib already exists. Skipping download.\n",
      "Processing month: 02...\n",
      "File temp_weather_data/2023/02/Madrid_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Barcelona_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Seville_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Valencia_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Castile-La Mancha (Wind)_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Aragon (Wind)_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Andalusia (Wind)_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Escatrón-Chiprana-Samper (Solar)_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Talasol Solar (Solar)_2023-02.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/02/Talayuela Solar (Solar)_2023-02.grib already exists. Skipping download.\n",
      "Processing month: 03...\n",
      "File temp_weather_data/2023/03/Madrid_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Barcelona_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Seville_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Valencia_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Castile-La Mancha (Wind)_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Aragon (Wind)_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Andalusia (Wind)_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Escatrón-Chiprana-Samper (Solar)_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Talasol Solar (Solar)_2023-03.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/03/Talayuela Solar (Solar)_2023-03.grib already exists. Skipping download.\n",
      "Processing month: 04...\n",
      "File temp_weather_data/2023/04/Madrid_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Barcelona_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Seville_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Valencia_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Castile-La Mancha (Wind)_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Aragon (Wind)_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Andalusia (Wind)_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Escatrón-Chiprana-Samper (Solar)_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Talasol Solar (Solar)_2023-04.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/04/Talayuela Solar (Solar)_2023-04.grib already exists. Skipping download.\n",
      "Processing month: 05...\n",
      "File temp_weather_data/2023/05/Madrid_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Barcelona_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Seville_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Valencia_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Castile-La Mancha (Wind)_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Aragon (Wind)_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Andalusia (Wind)_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Escatrón-Chiprana-Samper (Solar)_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Talasol Solar (Solar)_2023-05.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/05/Talayuela Solar (Solar)_2023-05.grib already exists. Skipping download.\n",
      "Processing month: 06...\n",
      "File temp_weather_data/2023/06/Madrid_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Barcelona_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Seville_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Valencia_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Castile-La Mancha (Wind)_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Aragon (Wind)_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Andalusia (Wind)_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Escatrón-Chiprana-Samper (Solar)_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Talasol Solar (Solar)_2023-06.grib already exists. Skipping download.\n",
      "File temp_weather_data/2023/06/Talayuela Solar (Solar)_2023-06.grib already exists. Skipping download.\n",
      "Processing month: 07...\n",
      "File temp_weather_data/2023/07/Madrid_2023-07.grib already exists. Skipping download.\n",
      "Requesting data for Barcelona (41.3851, 2.1734) for month 07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:15:23,792 INFO Request ID is 61ae0ccb-4d1d-49f5-abe0-db9633427aeb\n",
      "2024-12-09 16:15:23,926 INFO status has been updated to accepted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 57\u001b[0m\n\u001b[1;32m     47\u001b[0m request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m: variables,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m: [lat \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m, lon \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.1\u001b[39m, lat \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.1\u001b[39m, lon \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m],\n\u001b[1;32m     55\u001b[0m }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     c\u001b[38;5;241m.\u001b[39mretrieve(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreanalysis-era5-land\u001b[39m\u001b[38;5;124m\"\u001b[39m, request, file_name)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/datapi/legacy_api_client.py:169\u001b[0m, in \u001b[0;36mLegacyApiClient.retrieve\u001b[0;34m(self, name, request, target)\u001b[0m\n\u001b[1;32m    167\u001b[0m submitted: Remote \u001b[38;5;241m|\u001b[39m Results\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_until_complete:\n\u001b[0;32m--> 169\u001b[0m     submitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msubmit_and_wait_on_results(\n\u001b[1;32m    170\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     submitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    175\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest,\n\u001b[1;32m    177\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/datapi/api_client.py:458\u001b[0m, in \u001b[0;36mApiClient.submit_and_wait_on_results\u001b[0;34m(self, collection_id, **request)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit_and_wait_on_results\u001b[39m(\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m, collection_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest: Any\n\u001b[1;32m    444\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m datapi\u001b[38;5;241m.\u001b[39mResults:\n\u001b[1;32m    445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Submit a request and wait for the results to be ready.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    datapi.Results\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_api\u001b[38;5;241m.\u001b[39msubmit(collection_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\u001b[38;5;241m.\u001b[39mmake_results()\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/datapi/processing.py:489\u001b[0m, in \u001b[0;36mRemote.make_results\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, wait: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Results:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_on_results()\n\u001b[1;32m    490\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_api_response(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/datapi/processing.py:469\u001b[0m, in \u001b[0;36mRemote._wait_on_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_ready:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults not ready, waiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 469\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n\u001b[1;32m    470\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(sleep \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep_max)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "import xarray as xr\n",
    "\n",
    "# Load the configuration\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "locations = config[\"locations\"]\n",
    "variables = config[\"variables\"]\n",
    "times = config[\"time\"]  # Get the timeslots from the configuration file\n",
    "\n",
    "# Define the CDS API client\n",
    "c = cdsapi.Client()\n",
    "\n",
    "# Base directories for temporary and output files\n",
    "base_temp_dir = \"temp_weather_data\"\n",
    "base_output_dir = \"output_weather_data\"\n",
    "os.makedirs(base_temp_dir, exist_ok=True)\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Generate data for each month\n",
    "for month in range(1, 12):  # Loop through months 1 to 12\n",
    "    month_str = f\"{month:02}\"  # Ensure two-digit month format\n",
    "\n",
    "    # Determine the number of days in the current month\n",
    "    num_days = monthrange(2023, month)[1]\n",
    "    days = [f\"{day:02}\" for day in range(1, num_days + 1)]\n",
    "\n",
    "    print(f\"Processing month: {month_str}...\")\n",
    "\n",
    "    # Create a directory for this month\n",
    "    temp_dir = os.path.join(base_temp_dir, \"2023\", month_str)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Process each location\n",
    "    for location in locations:\n",
    "        lat, lon, name = location[\"latitude\"], location[\"longitude\"], location[\"name\"]\n",
    "        file_name = os.path.join(temp_dir, f\"{name}_2023-{month_str}.grib\")\n",
    "        \n",
    "        if not os.path.exists(file_name):  # Avoid re-downloading if file exists\n",
    "            print(f\"Requesting data for {name} ({lat}, {lon}) for month {month_str}...\")\n",
    "            request = {\n",
    "                \"variable\": variables,\n",
    "                \"year\": \"2023\",\n",
    "                \"month\": [month_str],\n",
    "                \"day\": days,  # Dynamically calculated days\n",
    "                \"time\": times,  # Time slots from configuration\n",
    "                \"format\": \"grib\",\n",
    "                \"area\": [lat + 0.1, lon - 0.1, lat - 0.1, lon + 0.1],\n",
    "            }\n",
    "            try:\n",
    "                c.retrieve(\"reanalysis-era5-land\", request, file_name)\n",
    "                print(f\"Data saved to {file_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving data for {name}, {month_str}: {e}\")\n",
    "        else:\n",
    "            print(f\"File {file_name} already exists. Skipping download.\")\n",
    "\n",
    "# Consolidate all downloaded data\n",
    "all_data = []\n",
    "\n",
    "# Process each month directory\n",
    "year_dir = os.path.join(base_temp_dir, \"2023\")\n",
    "for month_dir in sorted(os.listdir(year_dir)):\n",
    "    month_path = os.path.join(year_dir, month_dir)\n",
    "    if os.path.isdir(month_path):\n",
    "        print(f\"Processing month directory: {month_path}...\")\n",
    "        for file in os.listdir(month_path):\n",
    "            if file.endswith(\".grib\"):\n",
    "                file_path = os.path.join(month_path, file)\n",
    "                print(f\"Processing file: {file_path}...\")\n",
    "                try:\n",
    "                    ds = xr.open_dataset(file_path, engine=\"cfgrib\")\n",
    "                    df = ds.to_dataframe().reset_index()\n",
    "\n",
    "                    # Extract location name from file\n",
    "                    location_name = file.split(\"_2023-\")[0]\n",
    "\n",
    "                    # Add metadata\n",
    "                    df[\"location_name\"] = location_name\n",
    "\n",
    "                    all_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "if all_data:\n",
    "    print(\"Combining all data into a single DataFrame...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Save combined data to CSV\n",
    "    output_csv_file = os.path.join(base_output_dir, \"weather_data_2023.csv\")\n",
    "    combined_df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Consolidated weather data saved to {output_csv_file}.\")\n",
    "else:\n",
    "    print(\"No data to consolidate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading output_weather_data/weather_data_2023.csv...\n",
      "Processing data...\n",
      "Saving data by location to output_weather_data/weather_data_by_location_2023.json...\n",
      "Consolidating data by time...\n",
      "Saving consolidated data to output_weather_data/consolidated_weather_data_by_time_2023.csv...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "input_csv = \"output_weather_data/weather_data_2023.csv\"\n",
    "output_json = \"output_weather_data/weather_data_by_location_2023.json\"\n",
    "output_csv = \"output_weather_data/consolidated_weather_data_by_time_2023.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Convert time columns to datetime for easier processing\n",
    "df[\"valid_time\"] = pd.to_datetime(df[\"valid_time\"])\n",
    "\n",
    "# Initialize the main dictionary\n",
    "weather_data = {}\n",
    "\n",
    "# Group the data by location and valid_time\n",
    "print(\"Processing data...\")\n",
    "grouped = df.groupby([\"location_name\", \"valid_time\"])\n",
    "\n",
    "for (location, valid_time), group in grouped:\n",
    "    # Initialize the location's data if not already present\n",
    "    if location not in weather_data:\n",
    "        weather_data[location] = []\n",
    "\n",
    "    # Aggregate data for the time slot\n",
    "    aggregated_data = {\n",
    "        \"valid_time\": valid_time.isoformat(),\n",
    "        \"latitude\": group[\"latitude\"].mean(skipna=True),\n",
    "        \"longitude\": group[\"longitude\"].mean(skipna=True),\n",
    "        \"temperature\": group[\"t2m\"].mean(skipna=True),\n",
    "        \"solar_radiation\": group[\"ssr\"].mean(skipna=True),\n",
    "        \"wind_u_component\": group[\"u10\"].mean(skipna=True),\n",
    "        \"wind_v_component\": group[\"v10\"].mean(skipna=True),\n",
    "        \"surface_pressure\": group[\"sp\"].mean(skipna=True),\n",
    "        \"total_precipitation\": group[\"tp\"].mean(skipna=True),\n",
    "    }\n",
    "    weather_data[location].append(aggregated_data)\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "print(f\"Saving data by location to {output_json}...\")\n",
    "with open(output_json, \"w\") as json_file:\n",
    "    json.dump(weather_data, json_file, indent=4)\n",
    "\n",
    "# Consolidate data by valid_time\n",
    "print(\"Consolidating data by time...\")\n",
    "consolidated_data = {}\n",
    "\n",
    "# Process each location\n",
    "for location, records in weather_data.items():\n",
    "    for record in records:\n",
    "        valid_time = record[\"valid_time\"]\n",
    "\n",
    "        # Ensure the valid_time exists in the consolidated data\n",
    "        if valid_time not in consolidated_data:\n",
    "            consolidated_data[valid_time] = {}\n",
    "\n",
    "        # Flatten location-specific data into consolidated_data\n",
    "        consolidated_data[valid_time].update({\n",
    "            f\"{location}_latitude\": record[\"latitude\"],\n",
    "            f\"{location}_longitude\": record[\"longitude\"],\n",
    "            f\"{location}_temperature\": record[\"temperature\"],\n",
    "            f\"{location}_solar_radiation\": record[\"solar_radiation\"],\n",
    "            f\"{location}_wind_u_component\": record[\"wind_u_component\"],\n",
    "            f\"{location}_wind_v_component\": record[\"wind_v_component\"],\n",
    "            f\"{location}_surface_pressure\": record[\"surface_pressure\"],\n",
    "            f\"{location}_total_precipitation\": record[\"total_precipitation\"],\n",
    "        })\n",
    "\n",
    "# Convert the consolidated dictionary into a DataFrame\n",
    "df_consolidated = pd.DataFrame.from_dict(consolidated_data, orient=\"index\").reset_index()\n",
    "\n",
    "# Rename the index column to \"valid_time\"\n",
    "df_consolidated.rename(columns={\"index\": \"valid_time\"}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "print(f\"Saving consolidated data to {output_csv}...\")\n",
    "df_consolidated.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The current process is above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from output_weather_data/weather_data_2023.csv...\n",
      "Processing data...\n",
      "Saving processed data to temp/2023_weather_data_by_location.json...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/2023_weather_data_by_location.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save the processed data to a JSON file\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving processed data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_json_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_json_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     47\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(weather_data, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Consolidate the data by `valid_time` for the CSV output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/2023_weather_data_by_location.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "year = 2023\n",
    "base_dir = \"temp\"\n",
    "\n",
    "# Initialize the main dictionary\n",
    "weather_data = {}\n",
    "\n",
    "# Iterate through all months\n",
    "for month in range(1, 13):\n",
    "    month_dir = os.path.join(base_dir, str(year), f\"{month:02d}\")\n",
    "    month_file = os.path.join(month_dir, \"weather_data.csv\")\n",
    "    \n",
    "    # Check if the month's file exists\n",
    "    if not os.path.exists(month_file):\n",
    "        print(f\"Skipping {month_file}: File not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load the CSV file for the month\n",
    "    print(f\"Processing {month_file}...\")\n",
    "    df = pd.read_csv(month_file)\n",
    "\n",
    "    # Convert time columns to datetime for easier processing\n",
    "    df[\"valid_time\"] = pd.to_datetime(df[\"valid_time\"])\n",
    "\n",
    "    # Group the data by location\n",
    "    for location, group in df.groupby(\"location_name\"):\n",
    "        # Initialize the location's data if not already present\n",
    "        if location not in weather_data:\n",
    "            weather_data[location] = []\n",
    "\n",
    "        # Drop duplicate time slots by keeping the first non-NaN row for each `valid_time`\n",
    "        group = group.sort_values(\"valid_time\").dropna(subset=[\"t2m\", \"ssr\", \"u10\", \"v10\", \"sp\", \"tp\"], how='all')\n",
    "        group = group.groupby(\"valid_time\").first().reset_index()\n",
    "\n",
    "        # Process each row in the location's data\n",
    "        for _, row in group.iterrows():\n",
    "            # Create a dictionary for the current time slot\n",
    "            time_slot_data = {\n",
    "                \"valid_time\": row[\"valid_time\"].isoformat(),\n",
    "                \"latitude\": row[\"latitude\"],\n",
    "                \"longitude\": row[\"longitude\"],\n",
    "                \"temperature\": row[\"t2m\"],\n",
    "                \"solar_radiation\": row[\"ssr\"],\n",
    "                \"wind_u_component\": row[\"u10\"],\n",
    "                \"wind_v_component\": row[\"v10\"],\n",
    "                \"surface_pressure\": row[\"sp\"],\n",
    "                \"total_precipitation\": row[\"tp\"],\n",
    "            }\n",
    "            weather_data[location].append(time_slot_data)\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "output_file = os.path.join(base_dir, f\"{year}_weather_data_by_location.json\")\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(weather_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Weather data organized by location saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated weather data by time saved to consolidated_weather_data_by_time.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "input_file = \"weather_data_by_location.json\"\n",
    "output_csv_file = \"consolidated_weather_data_by_time.csv\"\n",
    "\n",
    "with open(input_file, \"r\") as json_file:\n",
    "    weather_data = json.load(json_file)\n",
    "\n",
    "# Create a dictionary to hold data for the DataFrame\n",
    "consolidated_data = {}\n",
    "\n",
    "# Process each location\n",
    "for location, records in weather_data.items():\n",
    "    for record in records:\n",
    "        valid_time = record[\"valid_time\"]\n",
    "\n",
    "        # Ensure the valid_time exists in the consolidated data\n",
    "        if valid_time not in consolidated_data:\n",
    "            consolidated_data[valid_time] = {}\n",
    "\n",
    "        # Flatten location-specific data into consolidated_data\n",
    "        consolidated_data[valid_time].update({\n",
    "            f\"{location}_latitude\": record[\"latitude\"],\n",
    "            f\"{location}_longitude\": record[\"longitude\"],\n",
    "            f\"{location}_temperature\": record[\"temperature\"],\n",
    "            f\"{location}_solar_radiation\": record[\"solar_radiation\"],\n",
    "            f\"{location}_wind_u_component\": record[\"wind_u_component\"],\n",
    "            f\"{location}_wind_v_component\": record[\"wind_v_component\"],\n",
    "            f\"{location}_surface_pressure\": record[\"surface_pressure\"],\n",
    "            f\"{location}_total_precipitation\": record[\"total_precipitation\"],\n",
    "        })\n",
    "\n",
    "# Convert the consolidated dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(consolidated_data, orient=\"index\").reset_index()\n",
    "\n",
    "# Rename the index column to \"valid_time\"\n",
    "df.rename(columns={\"index\": \"valid_time\"}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Consolidated weather data by time saved to {output_csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation shit I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading output_weather_data/weather_data_2023.csv...\n",
      "Processing data...\n",
      "Saving data by location to output_weather_data/weather_data_by_location_2023.json...\n",
      "Consolidating data by time...\n",
      "Saving consolidated data to output_weather_data/consolidated_weather_data_by_time_2023.csv...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "input_csv = \"output_weather_data/weather_data_2023.csv\"\n",
    "output_json = \"output_weather_data/weather_data_by_location_2023.json\"\n",
    "output_csv = \"output_weather_data/consolidated_weather_data_by_time_2023.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "print(f\"Loading {input_csv}...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Convert time columns to datetime for easier processing\n",
    "df[\"valid_time\"] = pd.to_datetime(df[\"valid_time\"])\n",
    "\n",
    "# Initialize the main dictionary\n",
    "weather_data = {}\n",
    "\n",
    "# Group the data by location and valid_time\n",
    "print(\"Processing data...\")\n",
    "grouped = df.groupby([\"location_name\", \"valid_time\"])\n",
    "\n",
    "for (location, valid_time), group in grouped:\n",
    "    # Initialize the location's data if not already present\n",
    "    if location not in weather_data:\n",
    "        weather_data[location] = []\n",
    "\n",
    "    # Aggregate data for the time slot\n",
    "    aggregated_data = {\n",
    "        \"valid_time\": valid_time.isoformat(),\n",
    "        \"latitude\": group[\"latitude\"].mean(skipna=True),\n",
    "        \"longitude\": group[\"longitude\"].mean(skipna=True),\n",
    "        \"temperature\": group[\"t2m\"].mean(skipna=True),\n",
    "        \"solar_radiation\": group[\"ssr\"].mean(skipna=True),\n",
    "        \"wind_u_component\": group[\"u10\"].mean(skipna=True),\n",
    "        \"wind_v_component\": group[\"v10\"].mean(skipna=True),\n",
    "        \"surface_pressure\": group[\"sp\"].mean(skipna=True),\n",
    "        \"total_precipitation\": group[\"tp\"].mean(skipna=True),\n",
    "    }\n",
    "    weather_data[location].append(aggregated_data)\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "print(f\"Saving data by location to {output_json}...\")\n",
    "with open(output_json, \"w\") as json_file:\n",
    "    json.dump(weather_data, json_file, indent=4)\n",
    "\n",
    "# Consolidate data by valid_time\n",
    "print(\"Consolidating data by time...\")\n",
    "consolidated_data = {}\n",
    "\n",
    "# Process each location\n",
    "for location, records in weather_data.items():\n",
    "    for record in records:\n",
    "        valid_time = record[\"valid_time\"]\n",
    "\n",
    "        # Ensure the valid_time exists in the consolidated data\n",
    "        if valid_time not in consolidated_data:\n",
    "            consolidated_data[valid_time] = {}\n",
    "\n",
    "        # Flatten location-specific data into consolidated_data\n",
    "        consolidated_data[valid_time].update({\n",
    "            f\"{location}_latitude\": record[\"latitude\"],\n",
    "            f\"{location}_longitude\": record[\"longitude\"],\n",
    "            f\"{location}_temperature\": record[\"temperature\"],\n",
    "            f\"{location}_solar_radiation\": record[\"solar_radiation\"],\n",
    "            f\"{location}_wind_u_component\": record[\"wind_u_component\"],\n",
    "            f\"{location}_wind_v_component\": record[\"wind_v_component\"],\n",
    "            f\"{location}_surface_pressure\": record[\"surface_pressure\"],\n",
    "            f\"{location}_total_precipitation\": record[\"total_precipitation\"],\n",
    "        })\n",
    "\n",
    "# Convert the consolidated dictionary into a DataFrame\n",
    "df_consolidated = pd.DataFrame.from_dict(consolidated_data, orient=\"index\").reset_index()\n",
    "\n",
    "# Rename the index column to \"valid_time\"\n",
    "df_consolidated.rename(columns={\"index\": \"valid_time\"}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "print(f\"Saving consolidated data to {output_csv}...\")\n",
    "df_consolidated.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files_path = glob.glob(r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 2\\Semester 1\\Deep Learning\\Project\\deeplearning\\data_23\"+'/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours recorded in marginalpdbc_20230326.1.csv: 23\n",
      "Hours recorded in marginalpdbc_20231029.1.csv: 25\n",
      "Hours recorded in precios_pibcic_20230326.1.csv: 23\n",
      "Hours recorded in precios_pibcic_20231029.1.csv: 25\n"
     ]
    }
   ],
   "source": [
    "data_da = pd.DataFrame()\n",
    "data_id = pd.DataFrame()\n",
    "corrupted_data = []\n",
    "\n",
    "for data_file_path in data_files_path:\n",
    "\n",
    "    try:\n",
    "        single_day_data = pd.read_csv(data_file_path, sep=';', skiprows=1, header=None, encoding = 'latin-1')\n",
    "        single_day_data = single_day_data.iloc[:-1,:-1] # filtering out last row and columns which are blank\n",
    "\n",
    "        if data_file_path.split('_')[-2] != 'pibcic':\n",
    "    \n",
    "            single_day_data.columns = ['Year','Month','Day','Hour','DA PT','DA ES'] # changing column names\n",
    "            \n",
    "            cols = list(range(4)) + [5] #selecting only dates and Spain data columns\n",
    "            \n",
    "\n",
    "            if len(single_day_data) == 24:\n",
    "                data_da = pd.concat([data_da,single_day_data.iloc[:,cols]])\n",
    "            elif len(single_day_data) > 24:\n",
    "                data_da = pd.concat([data_da,single_day_data.iloc[:-1,cols]])\n",
    "            elif len(single_day_data) < 24:\n",
    "                hour_to_add = single_day_data.iloc[-1].copy()\n",
    "                hour_to_add.iloc[4:] = np.nan\n",
    "                hour_to_add.iloc[3] = 24\n",
    "                single_day_data_corrected = pd.concat([single_day_data,hour_to_add.to_frame().T])\n",
    "                data_da = pd.concat([data_da,single_day_data_corrected.iloc[:,cols]])\n",
    "\n",
    "        else:\n",
    "            single_day_data.columns = ['Year','Month','Day','Hour','MaxES','MaxPT','MaxMO','MinES','MinPT','MinMO','AvgES','AvgPT','AvgMO']\n",
    "            single_day_data = single_day_data.iloc[1:] # removing first row which are old headers not values\n",
    "            cols = list(range(5)) + [7, 10] #selecting only dates and Spain data columns\n",
    "\n",
    "            if len(single_day_data) == 24:\n",
    "                data_id = pd.concat([data_id,single_day_data.iloc[:,cols]])\n",
    "            elif len(single_day_data) > 24:\n",
    "                data_id = pd.concat([data_id,single_day_data.iloc[:-1,cols]])\n",
    "            elif len(single_day_data) < 24:\n",
    "                # copying last hour data to missing hour 24\n",
    "                hour_to_add = single_day_data.iloc[-1].copy()\n",
    "                hour_to_add.iloc[4:] = np.nan\n",
    "                hour_to_add.iloc[3] = 24\n",
    "                single_day_data_corrected = pd.concat([single_day_data,hour_to_add.to_frame().T])\n",
    "                data_id = pd.concat([data_id,single_day_data_corrected.iloc[:,cols]])\n",
    "\n",
    "        if len(single_day_data) != 24:\n",
    "            file = data_file_path.split(\"\\\\\")[-1]\n",
    "            print(f'Hours recorded in {file}:', len(single_day_data))\n",
    "            corrupted_data.append(single_day_data)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(single_day_data)\n",
    "        file = data_file_path.split('\\\\')[-1]\n",
    "        print(f'Error in file {file}')\n",
    "\n",
    "# Intraday market dataset given with commas - changing to '.' float\n",
    "data_id.iloc[:,4:] = data_id.iloc[:,4:].replace(',','.',regex=True).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023.0, 1.0, 1.0, 1.0, 0.0]\n",
      "[2023.0, 1.0, 1.0, 1.0, 0.14, -4.0, -0.72]\n"
     ]
    }
   ],
   "source": [
    "# Changing str columns to floats\n",
    "str_columns_da = [column for column in data_da.columns if type(data_da[column].iloc[0])==str]\n",
    "str_columns_id = [column for column in data_id.columns if type(data_id[column].iloc[0])==str]\n",
    "\n",
    "for column in str_columns_da:\n",
    "    data_da[column] = data_da[column].astype(float)\n",
    "for column in str_columns_id:\n",
    "    data_id[column] = data_id[column].astype(float)\n",
    "\n",
    "# checking all columns are floats:\n",
    "print([data_da[column].iloc[0] for column in data_da.columns])\n",
    "print([data_id[column].iloc[0] for column in data_id.columns])\n",
    "\n",
    "# saving both DA and ID data in csv files\n",
    "data_da.to_csv(r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 2\\Semester 1\\Deep Learning\\Project\\deeplearning\\models_santi\\data_santi\\data_da.csv\", index=False)\n",
    "data_id.to_csv(r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 2\\Semester 1\\Deep Learning\\Project\\deeplearning\\models_santi\\data_santi\\data_id.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
